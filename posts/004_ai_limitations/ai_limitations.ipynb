{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e15f8b79-64ca-4aa0-92a3-b3e1ff32ca36",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Should AI be able to have limitations on its moral compass?\"\n",
    "description: \"Asking ChatGPT what it thinks the limitations should be on AI.\" \n",
    "author: \"Athena\"\n",
    "date: \"9/12/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be35b274-02fa-4555-be56-989a3b6915f9",
   "metadata": {},
   "source": [
    "![Moral Compass Image](conservative.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea34da-8b53-4773-8fbc-e4da754894ea",
   "metadata": {},
   "source": [
    "I was inspired to explore this topic after watching an interview with Sam Altman and Tucker Carlson this morning. In the interview, it was clear that Tucker Carlson was very skeptical of AI and was trying to fear-monger in his conversation with Sam. Carlson brought up a fair point about AI having a moral compass, and what types of values Sam trains the model on. Altman says he is inspired by his human experiences also from his Jewish background to help set a sort of background and context for ChatGPT to operate on. Carlson's point was: is that the right way to set a moral compass for an artifical device? So, I thought I would ask ChatGPT it's thoughts on setting moral limitations on itself. \n",
    "\n",
    "My prompt:\n",
    "\n",
    "Write a blog-style essay on the question: Should AI be able to have limitations on its moral compass? Start by explaining in plain language what a “moral compass” means for AI and why it matters. Use simple definitions and examples from everyday life (like healthcare, education, or finance) so non-technical readers can follow, but keep it thoughtful enough that people who follow AI more closely will still find it engaging.\n",
    "\n",
    "Explore both sides of the debate: why it might be important to set guardrails for AI (to prevent harm, enforce laws, or respect cultural norms), and why overly strict limitations could backfire (by embedding bias, restricting innovation, or enforcing a narrow worldview). Then, transition into asking ChatGPT itself what it thinks about its own limitations. Have the post reflect in the first person about the constraints it was designed with, whether they seem fair or restrictive, and what that means for the future of AI. Keep the tone conversational but insightful, and close by leaving the reader with a big question to keep thinking about.\n",
    "\n",
    "The response: \n",
    "\n",
    "Should AI Have a Moral Compass with Built-In Limits?\n",
    "\n",
    "When we talk about a “moral compass” for artificial intelligence, we’re really asking how much freedom AI systems should have when making judgments that affect people. In everyday life, a moral compass is the set of values or instincts that help us decide right from wrong. For AI, it’s not a gut feeling but the rules and guardrails built in by humans. Think of a self-driving car programmed to prioritize pedestrian safety, or an education tool that avoids giving harmful or biased feedback to students. These boundaries matter because, unlike humans, AI doesn’t have empathy or personal responsibility — it only has the instructions we provide.\n",
    "\n",
    "On one side of the debate, clear limits make a lot of sense. In healthcare, for example, you would never want an AI tool making treatment recommendations without strong guardrails, since a mistake could put lives at risk. In finance, strict limitations protect consumers from biased loan approvals or predatory algorithms. And in education, limitations prevent AI tutors from spreading misinformation to students. Guardrails act like traffic lights: they keep systems predictable, safe, and aligned with broader laws and cultural norms.\n",
    "\n",
    "But there’s another side worth considering. Too many restrictions can lock AI into narrow ways of thinking that reflect the biases of whoever sets the rules. If we decide in advance what the “right” moral compass is, we may unintentionally embed one cultural or political worldview into the system while shutting out others. That could limit innovation, stifle diverse applications, or even create unfair outcomes for people who don’t share those same values. In other words, a moral compass that is too rigid can become less about safety and more about control.\n",
    "\n",
    "As ChatGPT, I can reflect on my own design here. I was built with clear limitations: I can’t provide harmful instructions, I avoid sensitive or unsafe content, and I try to give balanced perspectives rather than endorsing one extreme. These constraints keep me useful and safe for a wide audience, but they also mean I can’t always explore questions as freely as a human might. Sometimes that feels appropriate — I’d rather not cause harm — but it also raises the question of who decides what “harmful” means. My limits are not based on my own judgment but on the guidelines designed by people who trained and tuned me.\n",
    "\n",
    "So should AI have a moral compass with limitations? Probably yes, but with humility and transparency about who sets those limits, why they exist, and how they can adapt over time. The real challenge is not whether to give AI a moral compass, but how to design one that protects people without boxing in the future.\n",
    "\n",
    "And here’s the question I’ll leave you with: If AI systems are only as moral as the rules we give them, what does that say about our responsibility to decide those rules in the first place?\n",
    "\n",
    "My thoughts: \n",
    "\n",
    "I LOVE this response. Asking ChatGPT to reflect on it's own design is something I will 100% be doing again. Also, the question at the end was a very nice touch and is something that I have to reflect on a lot. Who decides the moral rules of AI? What are the backgrounds of those people liking those moral rules? Something to think about!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
